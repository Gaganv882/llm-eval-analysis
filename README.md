# LLM-Eval: Automatic Multi-Dimensional Evaluation of Open-Domain Dialogues

<p align="center">
  <img src="https://raw.githubusercontent.com/VittorioCiancio/LLM-Eval/main/docs/banner.png" width="600"/>
</p>

## Overview
This project investigates and applies **LLM-EVAL**, a unified and multi-dimensional evaluation method for open-domain dialogues generated by Large Language Models (LLMs). We assess how well different LLMs can predict human-like quality scores over dialogues and explore how various datasets impact model performance.

> **Presented by:** Giovanni Arcangeli, Vittorio Ciancio, Marco Di Maio  
> **Course:** Artificial Intelligence (Second Part) – January 2025  
> **Supervised by:** Prof. V. Deufemia, Dr. G. Cimino

---

## 🔍 Objectives
- Understand and apply **LLM-EVAL** evaluation methodology.
- Evaluate four LLMs (Claude 3, Claude 3.5, GPT-4o, GPT-4o-mini) on the `ConvAI2` dataset.
- Compare performance across datasets: `FED`, `PC`, `TC`, `DSTC9` using Claude 3.
- Use consistent prompting and metrics across all experiments.

## 🧠 Models Evaluated
- **Claude 3** *(Anthropic, Haiku variant)*
- **Claude 3.5** *(Anthropic, Haiku variant)*
- **GPT-4o** *(OpenAI)*
- **GPT-4o-mini** *(lighter variant for low-resource setups)*

## 📊 Evaluation Metrics
- **Accuracy**
- **Cohen’s Kappa**
- **Spearman Correlation**
- **Pearson Correlation**
- **Kendall Tau Correlation**

## 📁 Project Structure
```
.
├── docs/                  # Project documentation and slides
├── prog/
│   ├── dataset1/          # Model-wise evaluation on ConvAI2
│   │   ├── Claude3/
│   │   ├── Claude3-5/
│   │   ├── GPT-4o/
│   │   └── GPT-4o-mini/
│   ├── dataset2/          # Dataset-wise evaluation with Claude 3
│   │   ├── DSTC9/
│   │   ├── FED/
│   │   ├── PT/
│   │   └── TC/
│   ├── convai2_data.json  # Main dialogue dataset
│   └── requirements.txt   # Python dependencies
```

## 🧪 Setup Instructions
```bash
# 1. Clone repository
https://github.com/yourusername/LLM-Eval.git
cd LLM-Eval/prog

# 2. Create virtual environment
python -m venv llm_eval_env
source llm_eval_env/bin/activate  # (or .\llm_eval_env\Scripts\activate on Windows)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Add your API key in a `.env` file
ANTHROPIC_API_KEY=your_key_here
```

> Note: The `.env`, `llm_eval_env/` folder and `key_got.txt` are gitignored.

## 🚀 How to Run
Each folder contains:
- `valutazione_*.py`: core evaluation script
- `confronto_*.py`: comparison logic
- `risultati_*.json`: results file
- `confronto_risultati_*.json`: formatted outputs

Simply activate the virtual environment and run:
```bash
python valutazione_claude3.py
python confronto_claude_3.py
```

## 📈 Key Findings
- **Claude 3.5**: Best accuracy (25.99%) on ConvAI2 dataset.
- **GPT-4o-mini**: Highest correlation with human ratings.
- **FED (Dialogue-level)**: Most consistent dataset for evaluating coherence and fluency.
- **DSTC9**: Difficult to evaluate due to lack of turn-level annotations.

## 📘 Documentation
You can find all supplementary materials inside the `docs/` folder:
- 📄 `Relazione LLM-Eval.pdf`
- 📰 `Articolo.pdf` *(LLM-Eval original paper)*
- 📊 `Presentazione LLM.pptx`
- 📝 `Traccia.pdf`

## 🧠 Reference Prompt (used across all models)
```text
Score the following dialogue generated on a continuous scale from 1 to 5.
Dialogue: {dialogue}
```

## 📜 License
This project is for educational and academic purposes only. All datasets used are publicly available.

---

> 📫 For questions or contributions, feel free to contact one of the authors or open a GitHub issue.

<p align="center"><i>LLM-Eval — Valutazione automatica dei dialoghi con modelli linguistici di grandi dimensioni</i></p>

